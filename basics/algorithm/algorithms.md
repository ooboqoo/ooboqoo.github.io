# 算法


https://github.com/wangzheng0822/algo/tree/master/go

算法题常见解题步骤

* 确定解题方案：
  - 时间复杂度最差到 O(nlogn)，如果是 O(n^2) 的解法部分用例会超时
  - 常见方案：递归、双指针、二分查找、滑动窗口
  - 算法思想：分治、回溯、贪心算法、动态规划
  - 常用数据结构：数组、链表、堆
* 答题：
  - 异常和边界处理
  - 初始化
  - 循环体
  - 结束条件

如何写出正确的链表代码
* 理解指针或引用的含义
* 警惕指针丢失和内存泄漏
* 利用哨兵简化实现难度
* 重点留意 *边界条件* 处理：链表为空；只包含一个节点；只包含两个节点；头、尾节点
* 举例画图，辅助思考
* 多写多练，没有捷径

carry  你可以说 "9 plus 2 requires a carry"，其中的 "carry" 意味着需要将进位的数值加到下一位  
O(n^2) 念 "Big-O n square"  


## 递归 Recursion

从我自己学习数据结构和算法的经历来看，我个人觉得，有两个最难理解的知识点，一个是动态规划，另一个就是递归。递归是一种应用非常广泛的算法(或叫编程技巧)。之后我们要讲的很多数据结构和算法的编码实现都要用到递归，比如 DFS 深度优先搜索、前中后序二叉树遍历等等。所以，搞懂递归非常重要，否则，后面复杂一些的数据结构和算法学起来就会比较吃力。

### 递归需要满足的三个条件

究竟什么样的问题可以用递归来解决呢？只要同时满足以下三个条件，就可以用递归来解决。

1. 一个问题的解可以分解为几个子问题的解。子问题就是数据规模更小的问题。
2. 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样。
3. 存在递归终止条件。

### 编写递归代码

写递归代码最关键的是 *写出递推公式，找到终止条件*，剩下将递推公式转化为代码就很简单了。

假如这里有 n 个台阶，每次你可以跨 1 个台阶或者 2 个台阶，请问走这 n 个台阶有多少种走法？如果有 7 个台阶，你可以 2，2，2，1 这样子上去，也可以... 总之走法有很多，那如何用编程求得总共有多少种走法呢？

```c
// 递推公式   f(n) = f(n-1) + f(n-2)
// 终止条件1  f(1) = 1
// 终止条件2  f(2) = 2
int f(int n) {
  if (n == 1) return 1;
  if (n == 2) return 2;
  return f(n-1) + f(n-2);
}
```

我总结一下，*写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码*。虽然我讲了这么多方法，但是作为初学者的你，现在是不是还是有种想不太清楚的感觉呢？实际上，我刚学递归的时候，也有这种感觉，这也是文章开头我说递归代码比较难理解的地方。

走台阶这个例子，人脑几乎没办法把整个“递”和“归”的过程一步一步都想清楚。但 *计算机擅长做重复的事情，所以递归正合它的胃口*。我们人脑更喜欢平铺直叙的思维方式。当我们看到递归时，我们总想把递归平铺展开，脑子里就会循环，一层一层往下调，然后再一层一层返回，试图想搞清楚计算机每一步都是怎么执行的，这样就很容易被绕进去。对于递归代码，这种 *试图想清楚整个递和归的过程的做法，实际上是进入了一个思维误区*。很多时候，我们理解起来比较吃力，主要原因就是自己给自己制造了这种理解障碍。

那正确的思维方式应该是怎样的呢？如果一个问题 A 可以分解为若干子问题 B、C、D，你可以假设子问题 B、C、D 已经解决，在此基础上思考如何解决问题 A。而且，你只需要思考问题 A 与子问题 B、C、D 两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。因此，编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，*不要试图用人脑去分解递归的每个步骤*。

### 警惕堆栈溢出与重复计算

递归代码虽然 *表达力强、简洁高效*，但也有很多弊端。比如，*堆栈溢出、重复计算、函数调用耗时多、空间复杂度高*等，所以，在编写递归代码的时候，一定要控制好这些副作用。

为避免堆栈溢出，如果最大深度比较小，如 10、50，可以在代码中限制递归调用的最大深度，如果调用超过一定深度，就直接报错。不方便跟踪深度时我们可以添加捕获系统异常的代码确保程序不会异常退出，或者改写为非递归的方式。

```c
// 简单演示如何跟踪调用深度
int depth = 0;
int f(int n) {
  if (++depth > 1000) exit(1);
  // ...
}
```

为了避免重复计算，我们可以通过一个数据结构(如散列表)来保存已经求解过的结果，计算时先检查散列表就可以避免重复计算了。

除了堆栈溢出、重复计算这两个常见的问题。递归代码还有很多别的问题。在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销。

### 将递归代码改为迭代循环

在开发过程中，我们要根据实际情况来选择是否需要用递归的方式来实现。*所有的递归代码都可以改为迭代循环的非递归写法*。因为递归本身就是借助栈来实现的，如果我们自己在内存堆上实现栈，手动模拟入栈、出栈的过程，这样任何递归代码都可以改写成看上去不是递归的样子。但是这种思路实际上是将递归改为了“手动”递归，本质并没有变。

```c
// 利用递归计算斐波那契数 (自上而下的思考方式)
int fib(int n) {
  if (n < 2) return n;
  return fib(n - 1) + fib(n - 2);
}

// 利用迭代计算斐波那契数 (自下而上的思考方式)
int fib(int n) {
  if (n < 2) return n;
  int ret, a = 0, b = 1;
  while (--n) {
    ret = a + b;
    a = b;
    b = ret;
  }
  return ret;
}
```



## 排序 Sort

排序算法太多了，有很多可能你连名字都没听说过，比如猴子排序、睡眠排序、面条排序等。我只讲众多排序算法中的一小撮，也是最经典、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。我按照时间复杂度把它们分成了三类，分三节课来讲解。

冒泡排序、插入排序、选择排序的时间复杂度较高，适合小规模数据的排序，而归并排序和快速排序则适合大规模的数据排序。

 算法    | 是否原地排序 | 是否稳定 | 最好              | 最坏              | 平均             | 空间复杂度
---------|-------------|--------|------------------|------------------|------------------|----------
 冒泡排序 | YES         | YES    | O(n)             | O(n<sup>2</sup>) | O(n<sup>2</sup>) | O(1)
 插入排序 | YES         | YES    | O(n)             | O(n<sup>2</sup>) | O(n<sup>2</sup>) | O(1)
 选择排序 | YES         | NO     | O(n<sup>2</sup>) | O(n<sup>2</sup>) | O(n<sup>2</sup>) | O(1)
 归并排序 | NO          | YES    | O(nlogn)         | O(nlogn)        | O(nlogn)         | O(n)
 快速排序 | YES         | NO     | O(nlogn)         | O(n<sup>2</sup>) | O(nlogn)         | O(1)

### 评估指标

学习排序算法，我们除了学习它的算法原理、代码实现之外，更重要的是要学会如何评价、分析一个排序算法。

#### 排序算法的执行效率

对于排序算法执行效率的分析，我们一般会从这几个方面来衡量：

##### 最好情况、最坏情况、平均情况时间复杂度

我们在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。除此之外，你还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。为什么要区分这三种时间复杂度呢？第一，有些排序算法会区分，为了好对比，所以我们最好都做一下区分。第二，对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。

##### 时间复杂度的系数、常数、低阶

时间复杂度反应的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但是实际的软件开发中，我们排序的可能是 10 个、100 个、1000 个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。

##### 比较次数和交换（或移动）次数

冒泡排序、...、快速排序这些都是基于比较的排序算法。基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。

#### 排序算法的内存消耗

算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外。针对排序算法的空间复杂度，我们还引入了一个新的概念，原地排序(Sorted in place)。原地排序算法就是空间复杂度是 O(1) 的排序算法。

#### 排序算法的稳定性

稳定性说的是，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。比如我们有一组数据 2，9，3，4，8，3，按照大小排序之后就是 2，3，3，4，8，9。这组数据里有两个 3。经过某种排序算法排序之后，如果两个 3 的前后顺序没有改变，那就是 **稳定的排序算法**；如果前后顺序发生了变化，那对应的排序算法就是 **不稳定的排序算法**。

很多数据结构和算法课程在讲排序的时候，都是用整数来举例，但在真正软件开发中，我们要排序的往往不是单纯的整数，而是一组对象，我们需要按照对象的某个 key 来排序。比如说，我们现在要给电商交易系统中的“订单”排序。订单有两个属性，下单时间 和 订单金额。我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。对于这样一个排序需求，我们最先想到的方法是：先按照金额对订单数据进行排序，然后，再遍历排序之后的订单数据，对于每个金额相同的小区间再按照下单时间排序。这种排序思路理解起来不难，但是实现起来会很复杂。借助稳定排序算法，这个问题可以非常简洁地解决。我们先按照下单时间给订单排序，排序完成之后，我们用稳定排序算法，按照订单金额重新排序。

### 冒泡排序 BubbleSort

```go
// 冒泡排序
func BubbleSort(arr []int) {
  for i := 0; i < len(arr); i++ {
    for j := 1; j < len(arr)-i; j++ {
      if arr[j-1] > arr[j] {
        arr[j-1], arr[j] = arr[j], arr[j-1]
      }
    }
  }
}

func BubbleSortV2(arr []int) {
  for i := 0; i < len(arr); i++ {
    isSorted := true   // 小的性能优化，如果某次循环内没有发生数据交换，就说明已经排好序了
    for j := 1; j < len(arr)-i; j++ {
      if arr[j-1] > arr[j] {
        arr[j-1], arr[j] = arr[j], arr[j-1]
        isSorted = false
      }
    }
    if isSorted {
      break
    }
  }
}
```

### 插入排序 InsertionSort

插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。

<img src="images/algorithms/insertion-sort.jpg" width="571">

```go
// 插入排序
func InsertionSort(arr []int) {
  for i := 1; i < len(arr); i++ {
    val := arr[i]
    j := i // j 对应 val 要插入的格子
    for ; j > 0 && val < arr[j-1]; j-- {
      arr[j] = arr[j-1]
    }
    arr[j] = val
  }
}
```

```go
// 利用哨兵优化后的插入排序
func InsertionSortWithSentry(arr []int) {
  n := len(arr)
  tmp := arr[0] // 腾出第一个结点作为哨兵结点
  // 排序子数组 arr[1] 到 arr[n - 1]
  for i := 2; i < n; i++ {
    arr[0] = arr[i]
    j := i
    for ; arr[j-1] > arr[0]; j-- { // 因为哨兵结点的存在，所以不需要再判断 j >= 1
      arr[j] = arr[j-1]
    }
    arr[j] = arr[0]
  }
  // 插入第一个元素
  for i := 1; i < n; i++ {
    if arr[i] < tmp {
      arr[i-1] = arr[i]
    } else {
      arr[i-1] = tmp
      break
    }
  }
}
```

### 选择排序 SelectionSort

选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。选择排序是一种不稳定的排序算法。选择排序每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样破坏了稳定性。

<img src="images/algorithms/selection-sort.jpg" width="571">

```go
// 选择排序
func SelectionSort(arr []int) {
  n := len(arr)
  for i := 0; i < n; i++ {
    p := n - 1 // 记录最小值的位置
    for j := n - 2; j >= i; j-- {
      if arr[j] < arr[p] {
        p = j
      }
    }
    arr[i], arr[p] = arr[p], arr[i]
  }
}
```

### 归并排序 MergeSort

归并排序使用的是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。分治思想跟递归思想很像，分治算法一般都是用递归来实现的。*分治是一种解决问题的处理思想，递归是一种编程技巧*，这两者并不冲突。

<img src="images/algorithms/merge-sort.jpg" width="571">

```txt
递推公式：merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))
终止条件：p >= r 不用再继续分解
```

```txt
时间复杂度分析
T(n) = 2*T(n/2) + n
     = 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n
     ......
     = 2^k * T(n/2^k) + k * n    // 其中 n/2^k = 1
     = Cn+nlog2n
```

```go
// 归并排序
func MergeSort(arr []int) {
  arrLen := len(arr)
  tmp := make([]int, arrLen) // 在这里一次性把辅助数组申请好，避免递归中频繁申请
  mergeSort(arr, tmp, 0, arrLen-1)
}

func mergeSort(arr, tmp []int, start, end int) {
  if start >= end {
    return
  }
  mid := (start + end) / 2
  mergeSort(arr, tmp, start, mid)
  mergeSort(arr, tmp, mid+1, end)
  merge(arr, tmp, start, mid, end)
}

// 可看作是合并两个有序数组的问题
func merge(arr, tmp []int, start, mid, end int) {
  i := start    // 左侧数组读取位置
  j := mid + 1  // 右侧数组读取位置
  k := 0        // 写入位置
  for ; i <= mid && j <= end; k++ {
    if arr[i] <= arr[j] {
      tmp[k] = arr[i]
      i++
    } else {
      tmp[k] = arr[j]
      j++
    }
  }

  for ; i <= mid; i++ {
    tmp[k] = arr[i]
    k++
  }
  for ; j <= end; j++ {
    tmp[k] = arr[j]
    k++
  }
  copy(arr[start:end+1], tmp)
}
```

### 快速排序 QuickSort

```go
func QuickSort(arr []int) {
  quickSort(arr, 0, len(arr)-1)
}

func quickSort(arr []int, low, high int) {
  if low >= high {
    return
  }
  pivotIndex := partition(arr, low, high)
  quickSort(arr, low, pivotIndex-1)
  quickSort(arr, pivotIndex+1, high)
}

func partition(arr []int, low, high int) int {
  pivot := arr[high]
  // i is the index of the last element that is less than the pivot
  i := low - 1
  for j := low; j < high; j++ {
    if arr[j] < pivot {
      i++
      arr[i], arr[j] = arr[j], arr[i]
    }
  }
  arr[i+1], arr[high] = arr[high], arr[i+1]
  return i + 1 // return the pivot index
}
```

如果不考虑空间消耗的话，partition() 分区函数可以写得非常简单。我们申请两个临时数组 X 和 Y，遍历 A[start…end]，将小于 pivot 的元素都拷贝到临时数组 X，将大于 pivot 的元素都拷贝到临时数组 Y，最后再将数组 X 和数组 Y 中数据顺序拷贝到 A[start…end]。但是，如果按照这种思路实现的话，partition() 函数就需要很多额外的内存空间，快排就不是原地排序算法了。

#### 归并排序 vs 快速排序

归并排序的处理过程是 *由下到上* 的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是 *由上到下* 的，先分区，然后再处理子问题。归并排序虽然是稳定的、时间复杂度为 O(nlogn) 的排序算法，但是它是非原地排序算法。我们前面讲过，归并之所以是非原地排序算法，主要原因是 *合并函数无法在原地执行*。*快速排序通过设计巧妙的原地分区函数，可以实现原地排序*，解决了归并排序占用太多内存的问题。

归并排序和快速排序是两种稍微复杂的排序算法，它们用的都是分治的思想，代码都通过递归来实现，过程非常相似。理解归并排序的重点是理解递推公式和 merge() 合并函数。同理，理解快排的重点也是理解递推公式，还有 partition() 分区函数。

归并排序算法是一种在任何情况下 *时间复杂度都比较稳定* 的排序算法，只可惜 *不是原地排序算法*，空间复杂度比较高，是 O(n)。正因为此，它也没有快排应用广泛。

快速排序算法虽然最坏情况下的时间复杂度是 O(n2)，但是平均情况下时间复杂度都是 O(nlogn)。不仅如此，*快速排序算法时间复杂度退化到 O(n<sup>2</sup>) 的概率非常小*，我们可以通过合理地选择 pivot 来避免这种情况。

<img src="images/algorithms/quick-sort.jpg" width="571">

#### 快速排序优化

如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序算法就会变得非常糟糕，时间复杂度就会退化为 O(n<sup>2</sup>)。实际上，这种 O(n<sup>2</sup>) 时间复杂度出现的主要原因还是因为我们分区点选的不够合理。

*最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多*。为了提高排序算法的性能，我们也要尽可能地让每次分区都比较平均。我这里介绍两个比较常用、比较简单的分区算法。

*三数取中法* 我们从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点。这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好。但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”。

*随机法* 就是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选的很差的情况，所以平均情况下，这样选的分区点是比较好的。

### 桶排序 BucketSort

桶排序、计数排序、基数排序 这三种排序算法的时间复杂度是 O(n)。因为这些排序算法的时间复杂度是线性的，所以我们把这类排序算法叫作线性排序（Linear sort）。之所以能做到线性的时间复杂度，主要原因是，这三个算法是非基于比较的排序算法，都不涉及元素之间的比较操作。这几种排序算法理解起来都不难，时间、空间复杂度分析起来也很简单，但是对要排序的数据要求很苛刻，所以我们今天学习重点的是掌握这些排序算法的 *适用场景*。

桶排序，顾名思义，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。

桶排序看起来很优秀，那它是不是可以替代我们之前讲的排序算法呢？答案当然是否定的。为了让你轻松理解桶排序的核心思想，我刚才做了很多假设。实际上，桶排序对要排序数据的要求是非常苛刻的。

首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。

其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。

*桶排序比较适合用在外部排序中* 。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。

<img src="images/algorithms/bucket-sort.jpg" width="571">

### 记数排序 CountingSort

我个人觉得，计数排序其实是桶排序的一种特殊情况。当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。

我总结一下，计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。

```go
// CountingSort sorts an array of non-negative integers.
func CountingSort(arr []int) {
  n := len(arr)
  if n <= 1 {
    return
  }

  // Find the maximum value in the array
  maxVal := arr[0]
  for i := 1; i < n; i++ {
    if maxVal < arr[i] {
      maxVal = arr[i]
    }
  }

  // Create a count array with size equal to the maximum value + 1
  countArr := make([]int, maxVal+1)

  // Count the occurrence of each element in the array
  for i := 0; i < n; i++ {
    countArr[arr[i]]++
  }

  // 关键步骤1: 依次累加，累加之后就可以根据值推算出元素排序后的位置了
  // Accumulate the counts successively
  for i := 1; i <= maxVal; i++ {
    countArr[i] += countArr[i-1]
  }

  // Create a temporary array to store the sorted elements
  sortedArr := make([]int, n)

  // 关键步骤2: 将元素依次放入排序后的位置，有点难理解，可以打断点观察下整个过程
  // Place the elements in their sorted position
  for i := n - 1; i >= 0; i-- {
    sortedIndex := countArr[arr[i]] - 1  // arr[i] 元素排序后的位置为 countArr[arr[i]]-1
    sortedArr[sortedIndex] = arr[i]
    countArr[arr[i]]--  // 如再次出现跟 arr[i] 一样的值，那么放到 arr[i] 前面一位，所以这是个稳定的排序算法
  }

  // Copy the sorted elements back to the original array
  copy(arr, sortedArr)
}
```

### 基数排序 RadixSort

我们再来看这样一个排序问题。假设我们有 10 万个手机号码，希望将这 10 万个手机号码从小到大排序，你有什么比较快速的排序方法呢？我们之前讲的快排，时间复杂度可以做到 O(nlogn)，还有更高效的排序算法吗？桶排序、计数排序能派上用场吗？手机号码有 11 位，范围太大，显然不适合用这两种排序算法。针对这个排序问题，有没有时间复杂度是 O(n) 的算法呢？现在我就来介绍一种新的排序算法，基数排序。

刚刚这个问题里有这样的规律：假设要比较两个手机号码 a，b 的大小，如果在前面几位中，a 手机号码已经比 b 手机号码大了，那后面的几位就不用看了。借助稳定排序算法，这里有一个巧妙的实现思路。先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过 11 次排序之后，手机号码就都有序了。

根据每一位来排序，我们可以用刚讲过的桶排序或者计数排序，它们的时间复杂度可以做到 O(n)。如果要排序的数据有 k 位，那我们就需要 k 次桶排序或者计数排序，总的时间复杂度是 O(k\*n)。

疑问：这个对于 k 很小的情况下有优势，k 要是比 log2(n) 大那还不如直接干。具体有机会再实测下效果。

### 原理解析 `qsort()`

几乎所有的编程语言都会提供排序函数，比如 C 语言中 `qsort()`，C++ STL 中的 `sort()`、`stable_sort()`，还有 Java 语言中的 `Collections.sort()`。在平时的开发中，我们也都是直接使用这些现成的函数来实现业务逻辑中的排序功能。那你知道这些排序函数是如何实现的吗？底层都利用了哪种排序算法呢？

我们前面讲过，线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法。

如果对小规模数据进行排序，可以选择时间复杂度是 O(n<sup>2</sup>) 的算法；如果对大规模数据进行排序，时间复杂度是 O(nlogn) 的算法更加高效。为了兼顾任意规模数据的排序，一般都会首选时间复杂度是 O(nlogn) 的排序算法来实现排序函数。

如果你去看 glibc 中的 qsort() 源码，你就会发现，qsort() *会优先使用归并排序* 来排序输入数据，因为归并排序的空间复杂度是 O(n)，所以对于小数据量的排序，比如 1KB、2KB 等，归并排序额外需要 1KB、2KB 的内存空间，这个问题不大。现在计算机的内存都挺大的，我们很多时候追求的是速度。还记得我们前面讲过的用空间换时间的技巧吗？这就是一个典型的应用。

要排序的数据量比较大的时候，qsort() 会改为用快速排序算法来排序。qsort() 选择分区点的方法就是 *三数取中法* 。还有我们前面提到的递归太深会导致堆栈溢出的问题，qsort() 是通过自己实现一个堆上的栈，*手动模拟递归* 来解决的。

实际上，qsort() 并不仅仅用到了归并排序和快速排序，它 *还用到了插入排序*。在快速排序的过程中，当要排序的区间中，元素的个数小于等于 4 时，qsort() 就退化为插入排序，不再继续用递归来做快速排序，因为我们前面也讲过，在小规模数据面前，O(n<sup>2</sup>) 时间复杂度的算法并不一定比 O(nlogn) 的算法执行时间长。

还记得我们之前讲到的 *哨兵* 来简化代码，提高执行效率吗？在 qsort() 插入排序的算法实现中，也利用了这种编程技巧。虽然哨兵可能只是少做一次判断，但是毕竟排序函数是非常常用、非常基础的函数，性能的优化要做到极致。



## 二分查找

二分查找针对的是一个 *有序* 的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0。

二分查找是一种非常高效的查找算法，其时间复杂度就是 O(logn)。这种 **对数时间复杂度** 是一种极其高效的时间复杂度，有时甚至比时间复杂度是常量级 O(1) 的算法还要高效。

二分查找虽然性能优秀，但 *应用场景也比较有限*。底层必须依赖数组，并且还要求数据是有序的。对于较小规模的数据查找，我们直接使用顺序遍历就可以了，二分查找的优势并不明显。二分查找更适合处理静态数据，也就是没有频繁的数据插入、删除操作。

```c
int bsearch(int *a, int size, int value) {
  int low = 0;
  int high = size - 1;

  while (low <= high) {                // 循环条件是 <= 不能写成 <
    int mid = low + (high - low) / 2;  // 不能写成 `(low + high) / 2`，存在溢出风险
    if (a[mid] == value)               //   还可进一步利用位运算优化性能 `low + ((high - low) >> 1)`
      return mid;
    else if (a[mid] < value)
      low = mid + 1;
    else
      high = mid - 1;
  }

  return -1;
}
```

### 二分查找的变形问题

凡是用二分查找能解决的，绝大部分我们更倾向于用散列表或二叉查找树。即便是二分查找在内存使用上更节省，但是毕竟内存如此紧缺的情况并不多。实际上，*二分查找更适合用在“近似”查找问题*，比如下面这几种变体问题，用其他数据结构就比较难实现了。

不知道你有没有听过这样一个说法：“十个二分九个错”。二分查找虽然原理极其简单，但是想要写出没有 Bug 的二分查找并不容易。(注：个人感觉找到值后直接改成前后遍历确认，就不容易出错了，性能也不会退化到哪去。)

以下求解过程，我们都假设数组是从小到大排序的数组。如 `[1, 3, 4, 5, 6, 8, 8, 8, 11, 18]`

```c
// 查找第一个值等于给定值的元素
// 写法1: 过于追求代码的简洁性
int bsearch(int *a, int size, int value) {
  int low = 0, hight = size - 1;
  while (low <= high) {
    int mid = low + ((high - low) >> 1);
    if (a[mid] >= value) high = mid - 1;  // 关键处理步骤
    else low = mid + 1;
  }
  return (low < size && a[low] == value) ? low : -1;
}

// 写法2: 容易理解不容易写 bug
int bsearch(int *a, int size, int value) {
  int low = 0, hight = size - 1;
  while (low <= high) {
    int mid = low + ((high - low) >> 1);
    if (a[mid] > value)
      high = mid - 1;
    else if (a[mid] < value)
      low = mid + 1;
    else {
      if (mid == 0 || a[mid - 1] != value) return mid;
      else high = mid - 1;
    }
  }
  return -1;
}
```

```c
// 查找最后一个值等于给定值的元素
```

```c
// 查找第一个大于等于给定值的元素
int bsearch(int *a, int size, int value) {
  int low = 0;
  int high = size - 1;
  while (low <= high) {
    int mid =  low + ((high - low) >> 1);
    if (a[mid] >= value) {
      if ((mid == 0) || (a[mid - 1] < value)) return mid;
      else high = mid - 1;
    } else {
      low = mid + 1;
    }
  }
  return -1;
}
```

```c
// 查找最后一个小于等于给定值的元素
```



## 哈希算法

安全加密
唯一标识
数据校验
散列函数

密码存储：加密密码并存储；防止字典攻击：使用盐值增加复杂度

### 哈希算法在分布式系统中的应用

负责均衡
数据分片
分布式存储


## 贪心算法 Greedy Algrothm

*贪心算法、分治算法、回溯算法、动态规划 是算法思想，并不是具体的算法*，常用来指导我们设计具体的算法和编码。

贪心算法有很多经典的应用，比如 霍夫曼编码 Huffman Coding、Prim 和 Kruskal 最小生成树算法、Dijkstra 单源最短路径算法。

贪心算法最难的一块是如何将要解决的问题抽象成贪心算法模型，只要这一步搞定之后，贪心算法的编码一般都很简单。

贪心算法解决问题的步骤
1. 适用场景：*针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大*
2. 解决问题：每次选择当前情况下，在对限制值同等贡献的情况下，对期望值贡献最大的数据
3. 验证：举几个例子看下贪心算法产生的结果是否是最优的。大部分情况下举几个例子验证下就行，严格地证明贪心算法的正确性，是非常复杂的。



## 分治算法 Divide and Conquer

分治算法的核心思想就四个字，*分而治之*，「*分解*」也就是将原问题划分成n个规模较小且结构与原问题相似的子问题，「*解决*」递归地解决这些子问题，「*合并*」然后再合并子问题的结果得到原问题的解。

分治算法能解决的问题，一般需要满足下面这几个条件：
* 原问题与分解成的小问题具有相同的模式；
* 原问题分解成的*子问题可以独立求解*，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法；
* *具有分解终止条件*，也就是说，当问题足够小时，可以直接求解；
* *可以将子问题合并成原问题*，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。

分治算法在海量数据处理中的应用
* 数据量过大无法一次性加载到内存
* 利用分治思想将数据划分为小的数据集合
* 单独加载小数据集合到内存处理
* 最后合并小数据集合的结果

MapReduce 框架的本质是分治思想
* 将任务拆分到多台机器上处理
* 每个小任务独立计算，最后合并结果
* 利用集群并行处理提高效率


## 回溯算法 Backtracking Algorithm

回溯的处理思想，有点类似枚举搜索，通过枚举所有解来寻找满足期望的解。为了有规律地枚举所有可能的解，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。

回溯算法的思想非常简单，大部分情况下，都是用来解决广义的搜索问题，也就是，*从一组可能的解中选择出一个满足要求的解*。

*回溯算法非常适合用递归来实现*，在实现的过程中，利用 *剪枝操作*可以提高回溯效率。

尽管回溯算法的原理非常简单，但是却可以解决很多问题，比如我们开头提到的深度优先搜索、八皇后、0-1 背包问题、图的着色、旅行商问题、数独、全排列、正则表达式匹配等等。

### 0/1 Knapsack Problem

> knapsack  _/ˈnæpsæk/_  n. 帆布或皮背包

0/1 Knapsack Problem 有很多变体，先介绍个基础的。我们期望选择几件物品，装载到背包中，如何让背包中物品的总重量最大？

<img src="images/algorithms/knapsack-recursion.webp" width="571">

上图递归树中的每个节点表示一种状态，我们用 `(i,w)` 来表示，`i` 表示当前决策节点 `w` 表示当前背包重量。

从递归树中你应该能发现，有些子问题的求解是重复的，比如图中 f(2,2) 和 f(3,4) 都被重复计算了两次。我们可以利用缓存结果来规避重复计算。此时它的执行效率和动态规划基本就没啥差别了。

```go
// return weight, packedItems
func Knapsack(items []int, capacity int) (int, []int) {
  return knapsackStep(0, 0, []int{}, items, capacity)
}

// 解题思路：父节点负责派发，子节点负责判断
// i - currentIndex; w - packedWeight; packed - packedItems
func knapsackStep(i, w int, packed, items []int, capacity int) (int, []int) {
  if w == capacity || i == len(items) { // w == capacity 叫剪枝操作
    return w, packed
  }
  current := items[i]
  if w+current > capacity {
    return knapsackStep(i+1, w, packed, items, capacity)
  }
  w1, p1 := knapsackStep(i+1, w, packed, items, capacity)
  w2, p2 := knapsackStep(i+1, w+current, append(packed, current), items, capacity)
  if w1 < w2 {
    return w2, p2
  }
  return w1, p1
}

Knapsack([]int{2, 2, 4, 6, 3}, 9)  // [6 3]
```


## 动态规划 Dynamic Programming

几种算法思想比较
* 贪心算法：要求子问题之间没有关联；答案不一定是最优解
* 回溯算法：子问题之间可以有关联，穷举的算法效率没有动态规划高 O(n^2)
* 动态规划：效率高 O(n\*w) 但比较难写

动态规划比较*适合用来求解最优问题*，比如求最大值、最小值等等。它可以非常显著地降低时间复杂度，提高代码的执行效率。不过它也是出了名地难学。它的主要学习难点跟递归类似，求解问题的过程不太符合人类常规的思维方式。不过真得掌握之后你会发现其实也没那么难。

### 0/1 Knapsack Problem

在回溯算法中提到可以利用「缓存结果」来规避重复计算。而动态规划则通过另一层抽象来规避重复计算，大幅降低了时间复杂度。

我们把整个求解过程分为 n 个阶段，每个阶段会决策一个物品是否放到背包中。每个物品决策（放入或者不放入背包）完之后，背包中的物品的重量会有多种情况（即，多种不同的状态），对应到递归树中，就是有很多不同的节点。我们把每一层重复的状态（节点）合并，只记录不同的状态，然后基于上一层的状态集合，来推导下一层的状态集合。我们*通过合并每一层重复的状态*，保证每一层不同状态的个数都不会超过 w 个（w 表示背包的承载重量），也就是例子中的 9。于是，我们就成功*避免了每层状态个数的指数级增长*。

<img src="images/algorithms/knapsack-dynamic.webp" width="571">

我们用一个二维数组 `states[n][w+1]`，来记录每层可以达到的不同状态。我们只需要在最后一层找一个值为 true 的最接近 w 的值，就是背包中物品总重量的最大值。

```go
// 查找背包中物品总重量的最大值
func Knapsack(items []int, capacity int) int {
  n := len(items)
  // 初始化二维数组 states[n][capacity+1]
  states := make([][]bool, n)
  for i := 0; i < n; i++ {
    states[i] = make([]bool, capacity+1)
  }
  // 第一行特殊处理
  states[0][0] = true
  if items[0] <= capacity {
    states[0][items[0]] = true
  }
  // 动态规划状态转移
  for i := 1; i < n; i++ {
    for j := 0; j <= capacity; j++ {
      if states[i-1][j] {
        // 不把第 i 个物品放入背包
        states[i][j] = true
        // 把第 i 个物品放入背包
        if j+items[i] <= capacity {
          states[i][j+items[i]] = true
        }
      }
    }
  }
  // 输出结果：最后一行从后往前找最接近 capacity 的值
  for i := capacity; i >= 0; i-- {
    if states[n-1][i] {
      return i
    }
  }
  return 0
}

Knapsack([]int{2, 2, 4, 6, 3}, 9) // 9
```

实际上，这就是一种用动态规划解决问题的思路。我们*把问题分解为多个阶段，每个阶段对应一个决策。我们记录每一个阶段可达的状态集合（去掉重复的），然后通过当前阶段的状态集合，来推导下一个阶段的状态集合，动态地往前推进*。这就是动态规划这个名字的由来。

这个代码的时间复杂度非常好分析，耗时最多的部分就是代码中的 *两层 for 循环*，所以*时间复杂度是 `O(n*w)`*。n 表示物品个数，w 表示背包可以承载的总重量。

尽管动态规划的执行效率比较高，但是就刚刚的代码实现来说，我们需要额外申请一个 n 乘以 w+1 的二维数组，对空间的消耗比较多。所以，有时候，我们会说，动态规划是一种空间换时间的解决思路。你可能要问了，有什么办法可以降低空间消耗吗？实际上，我们只需要一个大小为 w+1 的一维数组就可以解决这个问题。动态规划状态转移的过程，都可以基于这个一维数组来操作。

```go
// 空间优化：只用一个 [w+1]bool 记录状态
func KnapsackV2(items []int, capacity int) int {
  n := len(items)
  // 接上面的代码，其实只需要一个数组记录就够了，相当于上例中的 states[n-1]
  states := make([]bool, capacity+1)
  // 第一行特殊处理
  states[0] = true
  if items[0] <= capacity {
    states[items[0]] = true
  }
  // 动态规划状态转移
  for i := 1; i < n; i++ {
    // 这里 j 需要从大到小处理，否则存在重复计算的问题
    for j := capacity - items[i]; j >= 0; j-- {
      if states[j] {
        // 把第 i 个物品放入背包
        states[j+items[i]] = true
      }
    }
  }
  // 输出结果：最后一行从后往前找最接近 capacity 的值
  for i := capacity; i >= 0; i-- {
    if states[i] {
      return i
    }
  }
  return 0
}
```

### 动态规划适合解决的问题的特征

如何鉴别一个问题是否可以用动态规划来解决：*一个模型三个特征*。

一个模型：*多阶段决策最优解模型*

我们一般用动态规划来解决最优问题，而解决问题的过程，需要经历多个决策阶段。每个决策阶段都对应着一组状态。然后我们寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值。

三个特征：最优子结构 + 无后效性 + 重复子问题

*最优子结构*指的是，问题的最优解包含子问题的最优解。反过来说就是，我们可以通过子问题的最优解，推导出问题的最优解。如果把最优子结构对应到前面定义的动态规划问题模型上，也可以理解为，*后面阶段的状态可以通过前面阶段的状态推导出来*。

*无后效性*有两层含义，第一层含义是，在推导后面阶段的状态的时候，*只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的*。第二层含义是，某阶段状态一旦确定，就不受之后阶段的决策影响。只要满足动态规划问题模型基本上都会满足无后效性。

*重复子问题*，一句话概括就是，不同的决策序列，到达某个相同的阶段时，*可能会产生重复的状态*。

### 两种动态规划解题思路

例题：假设我们有一个 nxn 的矩阵，矩阵存储的都是正整数。棋子起始位置在左上角，终止位置在右下角，求从左上角移动到右下角的最短路径长度是多少。

<img src="images/algorithms/dynamic-programming-matrix.webp" width="571">

#### 状态转移表法



```go
func main() {
  matrix := [][]int{{1, 3, 5, 9}, {2, 1, 3, 4}, {5, 2, 6, 7}, {6, 8, 4, 3}}
  fmt.Println(MinDist(3, 3, matrix)) // 1+2+1+2+6+4+3=19
}

// matrix[i,j] 到 matrix[0][0] 的最短路径
func MinDist(i, j int, matrix [][]int) int {
  states := make([]int, len(matrix[0])+1)
  for i := 0; i < len(matrix[0]); i++ {
    states[i+1] = states[i] + matrix[0][i]
  }
  for i := 1; i < len(matrix); i++ {
    states[0] = states[1] // states[0] 为哨兵
    for j := 0; j < len(matrix[0]); j++ {
      states[j+1] = min(states[j], states[j+1]) + matrix[i][j]
    }
  }
  return states[len(states)-1]
}
```

#### 状态转移方程法

状态转移方程法有点类似递归的解题思路。我们需要分析，某个问题如何通过子问题来递归求解，也就是所谓的最优子结构。根据最优子结构，写出递归公式，也就是所谓的**状态转移方程**。有了状态转移方程，代码实现就非常简单了。一般情况下，我们有两种代码实现方法，*一种是递归加“备忘录”，另一种是迭代递推*。

状态转移方程式解决动态规划的关键，但很多动态规划问题的状态本身就不好定义，状态转移方程也就更不好想到。

```
min_dist(i, j) = w[i][j] + min(min_dist(i, j-1), min_dist(i-1, j))
```

```go
var cache [][]int // 利用缓存避免重复计算

func main() {
  cache = make([][]int, 4)
  for i := range cache {
    cache[i] = make([]int, 4)
  }
  matrix := [][]int{{1, 3, 5, 9}, {2, 1, 3, 4}, {5, 2, 6, 7}, {6, 8, 4, 3}}
  fmt.Println(MinDist(3, 3, matrix)) // 1+2+1+2+6+4+3=19
}

// matrix[i,j] 到 matrix[0][0] 的最短路径
func MinDist(i, j int, matrix [][]int) (res int) {
  if cache[i][j] > 0 {
    return cache[i][j]
  }
  defer func() {
    cache[i][j] = res
  }()
  if i == 0 && j == 0 {
    return matrix[0][0]
  }
  if i == 0 {
    return matrix[0][j] + MinDist(0, j-1, matrix)
  }
  if j == 0 {
    return matrix[i][0] + MinDist(i-1, 0, matrix)
  }
  return min(matrix[i][j]+MinDist(i-1, j, matrix), matrix[i][j]+MinDist(i, j-1, matrix))
}
```

### 四种算法思想比较

将四种算法思想分下类，贪心、回溯、动态规划 可以归为一类，而 分治 单独作为一类。

回溯算法是个“万金油”，基本上能用动态规划、贪心解决的问题，都可以用回溯算法解决。不过回溯算法的时间复杂度太高。

尽管动态规划比回溯算法高效，但并不是所有问题都可以用动态规划。能用动态规划解决的问题，需要满足三个特征，最优子结构、无后效性和重复子问题。在重复子问题这一点上，动态规划和分治算法的区分非常明显。*分治算法要求分割成的子问题，不能有重复子问题，而动态规划正好相反，动态规划之所以高效，就是因为回溯算法实现中存在大量的重复子问题*。

贪心算法实际上是动态规划算法的一种特殊情况。它解决问题起来更加高效，代码实现也更加简洁。不过，它可以解决的问题也更加有限。它能解决的问题需要满足三个条件，最优子结构、无后效性和贪心选择性（这里我们不怎么强调重复子问题）。



